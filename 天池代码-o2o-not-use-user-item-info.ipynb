{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J06Y18FFcbDH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\dask\\dataframe\\utils.py:367: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\dask\\dataframe\\utils.py:367: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\dask\\dataframe\\utils.py:367: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'modin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpd\u001b[39;00m\n\u001b[0;32m     14\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'modin'"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6rjoe74ruSl"
   },
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "# pandarallel will determine how many cores you have, but you can specify it yourself\n",
    "pandarallel.initialize(progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyfdmSgKXU80"
   },
   "outputs": [],
   "source": [
    "path = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = pd.date_range('2014-12-13', '2014-12-18')\n",
    "date_list = [str(x).split(' ')[0] for x in date_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date_ in reversed(date_list):\n",
    "    print(date_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_id item_id behavior_type user_geohash item_category time\n",
    "# item_id item_category item_geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 大数据版本\n",
    "# for data_index in [0,1,2,3,4]:\n",
    "#     t = mpd.read_csv(path + '/data/round1_user_{}.txt'.format(data_index),sep='\\t',header=None)\n",
    "#     t['date'] = t[5].apply(lambda x: str(x).split(' ')[0])\n",
    "#     for date_ in date_list:\n",
    "#         print(date_)\n",
    "#         tt = t[t['date']==date_]\n",
    "#         print(tt['date'].unique(),date_,tt.shape)\n",
    "#         tt.columns = [str(x) for x in tt.columns]\n",
    "#         tt.to_parquet(path + \"/sub_data/by_date/round1_user_{}_{}.parquet\".format(date_, data_index))\n",
    "#     del t\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date_ in reversed(date_list):\n",
    "    print('begin',date_)\n",
    "    for data_index in [0, 1, 2, 3, 4]:\n",
    "        print(path + '/data/round1_user_{}.txt'.format(data_index))\n",
    "        tmp_round_user = pd.read_csv(path + '/data/round1_user_{}.txt'.format(data_index),\n",
    "                                     sep='\\t',\n",
    "                                     header=None,\n",
    "                                     chunksize=5000000\n",
    "                                     )\n",
    "        parquet_data = []\n",
    "        for t in tmp_round_user:\n",
    "            t['date'] = t[5].apply(lambda x: str(x).split(' ')[0])\n",
    "            t = t[t['date'] == date_]\n",
    "            parquet_data.append(t)\n",
    "        parquet_data = pd.concat(parquet_data,copy=False)\n",
    "        parquet_data.columns = [str(x) for x in parquet_data.columns]\n",
    "        parquet_data.to_parquet(path + \"/sub_data/by_date/round1_user_{}_{}.parquet\".format(date_, data_index))\n",
    "        print(parquet_data.shape)\n",
    "        print(parquet_data['date'].unique())\n",
    "        del parquet_data,t\n",
    "    del tmp_round_user\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BN0fyXqfeWRW"
   },
   "outputs": [],
   "source": [
    "# 1.读取12-17数据，保留12-17行为为4的数据，保存为train-label \n",
    "data_date = '2014-12-17'\n",
    "print('for train data set')\n",
    "train_label = []\n",
    "for data_index in [0, 1, 2, 3, 4]:\n",
    "    tmp_parquet = pd.read_parquet(path + \"/sub_data/by_date/round1_user_{}_{}.parquet\".format(data_date,data_index))\n",
    "#   user_id item_id behavior_type user_geohash item_category time\n",
    "    tmp_parquet = tmp_parquet[tmp_parquet['2'] == 4][['0', '1']]\n",
    "    print(tmp_parquet.shape)\n",
    "    train_label.append(tmp_parquet)\n",
    "    del tmp_parquet\n",
    "train_label = pd.concat(train_label,copy=False).drop_duplicates()\n",
    "train_label['label'] = 1\n",
    "train_label.columns = ['user_id', 'item_id', 'label']\n",
    "train_label.to_parquet(path + \"/sub_data/use_data/train_label.parquet\")\n",
    "print(train_label.shape)\n",
    "del train_label\n",
    "gc.collect()\n",
    "\n",
    "# 2.构造12-17日所有item item_category 构造候选数据集\n",
    "train_item = []\n",
    "for data_index in [0, 1, 2, 3, 4]:\n",
    "    tmp_parquet = pd.read_parquet(path + \"/sub_data/by_date/round1_user_{}_{}.parquet\".format(data_date,data_index))\n",
    "#   item_id item_category\n",
    "    tmp_parquet = tmp_parquet[['1', '4']]\n",
    "    print(tmp_parquet.shape)\n",
    "    train_item.append(tmp_parquet)\n",
    "    del tmp_parquet\n",
    "train_item = pd.concat(train_item,copy=False).drop_duplicates()\n",
    "train_item.columns = ['item_id', 'item_category']\n",
    "train_item.to_parquet(path + \"/sub_data/use_data/train_item.parquet\")\n",
    "print(train_item.shape)\n",
    "del train_item\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QN-Sfhx-eWaF"
   },
   "outputs": [],
   "source": [
    "# 1.读取12-18数据，保留12-18行为为4的数据，保存为valid-label,去重\n",
    "data_date = '2014-12-18'\n",
    "print('for valid data set')\n",
    "valid_label = []\n",
    "for data_index in [0, 1, 2, 3, 4]:\n",
    "    tmp_parquet = pd.read_parquet(path + \"/sub_data/by_date/round1_user_{}_{}.parquet\".format(data_date,data_index))\n",
    "    tmp_parquet = tmp_parquet[tmp_parquet['2'] == 4][['0', '1']]\n",
    "    print(tmp_parquet.shape)\n",
    "    valid_label.append(tmp_parquet)\n",
    "    del tmp_parquet\n",
    "valid_label = pd.concat(valid_label).drop_duplicates()\n",
    "valid_label['label'] = 1\n",
    "valid_label.columns = ['user_id', 'item_id', 'label']\n",
    "valid_label.to_parquet(path + \"/sub_data/use_data/valid_label.parquet\")\n",
    "print(valid_label.shape)\n",
    "del valid_label\n",
    "gc.collect()\n",
    "\n",
    "# 2.构造12-18日所有item item_category 构造候选数据集\n",
    "valid_item = []\n",
    "for data_index in [0, 1, 2, 3, 4]:\n",
    "    tmp_parquet = pd.read_parquet(path + \"/sub_data/by_date/round1_user_{}_{}.parquet\".format(data_date,data_index))\n",
    "    tmp_parquet = tmp_parquet[['1', '4']]\n",
    "    print(tmp_parquet.shape)\n",
    "    valid_item.append(tmp_parquet)\n",
    "    del tmp_parquet\n",
    "valid_item = pd.concat(valid_item).drop_duplicates()\n",
    "valid_item.columns = ['item_id', 'item_category']\n",
    "valid_item.to_parquet(path + \"/sub_data/use_data/valid_item.parquet\")\n",
    "print(valid_item.shape)\n",
    "del valid_item\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_history_data(label,item,user_data_list,result_name,is_not_submit):\n",
    "    feature = ['user_id', 'item_id', 'behavior_type', 'user_geohash', 'item_category', 'time']\n",
    "    # 读取历史区间的数据\n",
    "    data = []\n",
    "    # user_data_list = {'2014-12-16': 0, '2014-12-15': 1}\n",
    "    for use_date in user_data_list:\n",
    "        for data_index in [0, 1, 2, 3, 4]:\n",
    "            tmp_parquet = pd.read_parquet(path + \"/sub_data/by_date/round1_user_{}_{}.parquet\".format(use_date, data_index))\n",
    "            del tmp_parquet['date']\n",
    "            tmp_parquet.columns = feature\n",
    "            print(tmp_parquet.shape)\n",
    "            tmp_parquet = tmp_parquet[tmp_parquet['item_id'].isin(item['item_id'].unique())]\n",
    "            print(tmp_parquet.shape)\n",
    "            tmp_parquet['gap'] = tmp_parquet['time'].apply(lambda x: str(x).split(' ')[0])\n",
    "            tmp_parquet['hour'] = tmp_parquet['time'].apply(lambda x: int(str(x).split(' ')[1]))\n",
    "            tmp_parquet['gap'] = tmp_parquet['gap'].map(user_data_list)\n",
    "            del tmp_parquet['user_geohash']\n",
    "            del tmp_parquet['time']\n",
    "            data.append(tmp_parquet)\n",
    "            del tmp_parquet\n",
    "            gc.collect()\n",
    "    data = pd.concat(data,copy=False).reset_index(drop=True)\n",
    "    print(data.shape)\n",
    "    data.to_parquet(path + \"/sub_data/use_data/history_{}.parquet\".format(result_name))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_ptp(x):\n",
    "    return np.max(x) - np.min(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择gap区间，统计以key1为主键，key2为条件，是否全部行为bh内的 func特征\n",
    "#  get_windows_feat(data,1,'user_item','f',[1,2,3,4],['count'],False)\n",
    "#  get_windows_feat(data,1,'user_item','offset',[1,2,3,4],['min','max','nunique','var'],False)\n",
    "#  get_windows_feat(data,1,'user_item','gap',[1,2,3,4],['nunique'],True)\n",
    "def get_windows_feat(data,gap,key1,key2,bh,fnc,is_eq):\n",
    "    # 构造逆序时间信息\n",
    "    if key2 == 'offset':\n",
    "        data['offset'] = data['gap'] * 24 + 24 - data['hour']\n",
    "    if key2 == 'f':\n",
    "        data['f'] = 1\n",
    "    if is_eq:\n",
    "        tt = data[(data['gap']==gap)&(data['behavior_type'].isin(bh))].groupby(key1).agg({key2:fnc}).reset_index()\n",
    "        tt.columns = ['w_eq{}b{}_{}'.format(gap,''.join(map(str,bh)),key1) + x[0] +'_'+ x[1] if x[1]!='' else x[0] for x in tt.columns]\n",
    "    else:\n",
    "        tt = data[(data['gap']<=gap)&(data['behavior_type'].isin(bh))].groupby(key1).agg({key2:fnc}).reset_index() \n",
    "        tt.columns = ['w_lt{}b{}_{}'.format(gap,''.join(map(str,bh)),key1) + x[0] +'_'+ x[1] if x[1]!='' else x[0] for x in tt.columns]\n",
    "    tt = tt.fillna(0)\n",
    "#     tt = reduce_mem_usage(tt)\n",
    "    print(gap,tt.shape)\n",
    "    return tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要针对behavior_type信息做展开特征\n",
    "#  get_pivot_table_feature(data,1,'user_item','behavior_type',False)\n",
    "#  get_pivot_table_feature(data,1,'user_item','behavior_type',True)\n",
    "def get_pivot_table_feature(data,gap,key1,key2,is_eq):\n",
    "    def get_ctr(x):\n",
    "        x = x.values\n",
    "#         print(x)\n",
    "        res = [x[3]/(1+x[0]),x[3]/(1+x[1]),x[3]/(1+x[2]),x[2]/(1+x[0]),x[1]/(1+x[0])]\n",
    "        if x[0] == 0:\n",
    "            res[0] = -1\n",
    "            res[3] = -1\n",
    "            res[4] = -1\n",
    "        if x[1] == 0:\n",
    "            res[1] = -1\n",
    "        if x[2] == 0:\n",
    "            res[2] = -1\n",
    "        return res\n",
    "\n",
    "    if is_eq:\n",
    "        t = data[data['gap']==gap]\n",
    "    else:\n",
    "        t = data[data['gap']<=gap]\n",
    "    t['f'] = 1\n",
    "    t = pd.pivot_table(t,'f',key1,key2,aggfunc='sum', fill_value=0).reset_index()\n",
    "    # 多进程加速\n",
    "    t[['4_1','4_2','4_3','2_1','3_1']] = t[[1,2,3,4]].parallel_apply(lambda x:get_ctr(x),axis=1,result_type=\"expand\")\n",
    "    if is_eq:    \n",
    "        t.columns = ['weq{}ctr'.format(gap)+ '_' +key1 +'_' +key2 +'_' + str(x) if x != key1 else key1 for x in t.columns]\n",
    "    else:\n",
    "        t.columns = ['wlt{}ctr'.format(gap)+ '_' +key1 +'_' +key2 +'_' + str(x) if x != key1 else key1 for x in t.columns]       \n",
    "    t = t.fillna(0)\n",
    "#     t = reduce_mem_usage(t)\n",
    "    print(gap,t.shape)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_feature(data, item, label, is_submit, name):\n",
    "    data['user_category'] = data[['user_id','item_category']].parallel_apply(lambda x:str(x[0]) +'_' + str(x[1]),axis=1)\n",
    "#     data['item_hour'] = data[['item_id','hour']].parallel_apply(lambda x:str(x[0]) +'_' + str(x[1]),axis=1)\n",
    "#     data['user_item_hour'] = data[['user_item','hour']].parallel_apply(lambda x:str(x[0]) +'_' + str(x[1]),axis=1)\n",
    "\n",
    "        \n",
    "#     base_data = pd.concat([data[data['gap']==0],data[(data['gap']!=0)&(data['behavior_type']!=1)]],copy=False)\n",
    "#     print(data.shape)\n",
    "    base_data = data[data['item_id'].isin(list(item['item_id'].unique()))]\n",
    "    base_data = base_data[['user_id','item_id','item_category','user_item','user_category']].drop_duplicates()\n",
    "    print(base_data.shape)\n",
    "    print(data.head())\n",
    "    \n",
    "    print(data.shape)\n",
    "    data = data[data['item_id'].isin(list(item['item_id'].unique()))].reset_index(drop=True)\n",
    "    print(data.shape)\n",
    "    \n",
    "    print('========== user ==========')\n",
    "#  get_windows_feat(data,1,'user_item','f',[1,2,3,4],['count'],False)\n",
    "#  get_windows_feat(data,1,'user_item','offset',[1,2,3,4],['min','max','nunique','var'],False)\n",
    "#  get_windows_feat(data,1,'user_item','gap',[1,2,3,4],['nunique'],True)\n",
    "# 统计用户交互过的商品个数和数量 \n",
    "    for windows in [0]:\n",
    "        print('get_windows_feat user_id item_id 1,2,3,4 nunique count {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_u_1_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_u_1_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_windows_feat(data,windows,'user_id','item_id',[1,2,3,4],['nunique','count'],False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_u_1_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_id'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "        \n",
    "    for windows in [0]:\n",
    "        print('get_windows_feat user_id item_id 1,2,3,4 nunique count {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_u_104_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_u_104_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_windows_feat(data,windows,'user_id','item_id',[3,4],['nunique','count'],False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_u_104_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_id'],how='left',copy=False)\n",
    "        del t;gc.collect()  \n",
    "        \n",
    "# 统计用户交互过的商品个数和数量 \n",
    "    for windows in [0]:\n",
    "        print('get_windows_feat user_id item_id 4 nunique count {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_u_101_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_u_101_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_windows_feat(data,windows,'user_id','item_id',[4],['nunique','count'],False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_u_101_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_id'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "        \n",
    "# 统计用户交互过的商品类型个数和数量 \n",
    "    for windows in [0]:\n",
    "        print('get_windows_feat user_id item_category 1,2,3,4 nunique {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_u_201_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_u_201_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_windows_feat(data,windows,'user_id','item_category',[1,2,3,4],['count','nunique'],False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_u_201_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_id'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "\n",
    "    for windows in [0]:\n",
    "        print('get_windows_feat user_id item_category 1,2,3,4 nunique {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_u_204_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_u_204_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_windows_feat(data,windows,'user_id','item_category',[4],['count','nunique'],False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_u_204_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_id'],how='left',copy=False)\n",
    "        del t;gc.collect()        \n",
    "        \n",
    "# 统计用户不同行为的频次\n",
    "#  get_pivot_table_feature(data,1,'user_item','behavior_type',False)\n",
    "#  get_pivot_table_feature(data,1,'user_item','behavior_type',True)\n",
    "    for windows in [0]:\n",
    "        print('get_pivot_table_feature user_id behavior_type {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_u_3_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_u_3_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_pivot_table_feature(data,windows,'user_id','behavior_type',False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_u_3_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_id'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "\n",
    "    for windows in [0]:\n",
    "        print('get_pivot_table_feature user_id behavior_type {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_u_303_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_u_303_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_pivot_table_feature(data,windows,'user_id','hour',False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_u_303_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_id'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "\n",
    "# 统计用户在重构时间轴内的连续值特征\n",
    "#      get_windows_feat(data,1,'user_item','offset',[1,2,3,4],['min','max','nunique','var'],False)\n",
    "    for windows in [0]:\n",
    "        print('get_windows_feat user_id offset 1,2,3,4 max min {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_u_401_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_u_401_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_windows_feat(data,windows,'user_id','hour',[1,2,3,4],['max','min','mean','var'],False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_u_401_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_id'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "    print(base_data.shape)\n",
    "#      get_windows_feat(data,1,'user_item','offset',[1,2,3,4],['min','max','nunique','var'],False)\n",
    "    for windows in [0]:\n",
    "        print('get_windows_feat user_id offset 1,2,3,4 max min {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_u_5_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_u_5_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_windows_feat(data,windows,'user_id','offset',[4],['max','min'],False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_u_5_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_id'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "    print(base_data.shape)\n",
    "    \n",
    "    for windows in [0]:\n",
    "        print('get_windows_feat user_id offset 1,2,3,4 max min {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_u_6_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_u_6_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_windows_feat(data,windows,'user_id','hour',[4],['max','min'],False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_u_6_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_id'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "    print(base_data.shape)    \n",
    "    print('========== item ==========')\n",
    "# 统计商品交互过多少用户和频次\n",
    "    for windows in [0]:\n",
    "        print('get_windows_feat item_id user_id 1,2,3,4 nunique count {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_i_1_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_i_1_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_windows_feat(data,windows,'item_id','user_id',[1,2,3,4],['nunique','count'],False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_i_1_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['item_id'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "\n",
    "# 统计商品交互过多少用户和频次\n",
    "    for windows in [0]:\n",
    "        print('get_windows_feat item_id user_id 1,2,3,4 nunique count {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_i_104_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_i_104_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_windows_feat(data,windows,'item_id','user_id',[4],['nunique','count'],False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_i_104_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['item_id'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "        \n",
    "# 统计商品被购买点击加购物车的行为特征\n",
    "    for windows in [0]:\n",
    "        print('get_pivot_table_feature item_id behavior_type {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_i_2_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_i_2_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_pivot_table_feature(data,windows,'item_id','behavior_type',False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_i_2_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['item_id'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "    print(base_data.shape)\n",
    "    \n",
    "    for windows in [0]:\n",
    "        print('get_pivot_table_feature item_id behavior_type {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_i_3_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_i_3_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_pivot_table_feature(data,windows,'item_id','hour',False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_i_3_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['item_id'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "    print(base_data.shape)    \n",
    "    print('========== user item ==========')\n",
    "# 过去n天 用户-商品对 出现的统计特征 统计用户+商品的统计特征\n",
    "\n",
    "    for windows in [0]:\n",
    "        print('get_windows_feat  user_item f 1,2,3,4 count {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_ui_1_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_ui_1_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_windows_feat(data,windows,'user_item','f',[1,2,3,4],['count'],False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_ui_1_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_item'],how='left',copy=False)\n",
    "        del t;gc.collect()  \n",
    "        \n",
    "# 过去N天，user-item 的行为特征的展开表达\n",
    "    for windows in [0]:\n",
    "        print('get_pivot_table_feature user_item behavior_type {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_ui_2_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_ui_2_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_pivot_table_feature(data,windows,'user_item','behavior_type',False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_ui_2_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_item'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "\n",
    "    for windows in [0]:\n",
    "        print('get_pivot_table_feature user_item_hour behavior_type {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_ui_202_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_ui_202_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_pivot_table_feature(data,windows,'user_item','hour',False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_ui_202_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_item'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "        \n",
    "# user-item 最近三天内，用户与对应商品最近一次，最远一次出现的 gap        \n",
    "    for windows in [0]:\n",
    "        print('get_windows_feat user_item gap 1,2,3,4 max min {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_ui_3_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_ui_3_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_windows_feat(data,windows,'user_item','offset',[1,2,3,4],['max','min','nunique'],False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_ui_3_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_item'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "        \n",
    "    for windows in [0]:\n",
    "        print('get_windows_feat user_item gap 1,2,3,4 max min {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_ui_4_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_ui_4_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_windows_feat(data,windows,'user_item','hour',[1,2,3,4],['max','min'],False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_ui_4_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_item'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "# uc 特征        \n",
    "    for windows in [0]:\n",
    "        print('get_windows_feat user_item gap 1,2,3,4 max min {} False'.format(windows))\n",
    "        if os.path.exists(path + \"/sub_data/use_data/{}_uc_1_w_{}.parquet\".format(name,windows)):\n",
    "            t = pd.read_parquet(path + \"/sub_data/use_data/{}_uc_1_w_{}.parquet\".format(name,windows))\n",
    "        else:\n",
    "            t = get_windows_feat(data,windows,'user_category','f',[1,2,3,4],['count'],False)\n",
    "            print(t.head())\n",
    "            t.to_parquet(path + \"/sub_data/use_data/{}_uc_1_w_{}.parquet\".format(name,windows))\n",
    "        base_data = pd.merge(base_data,t,on=['user_category'],how='left',copy=False)\n",
    "        del t;gc.collect()\n",
    "\n",
    "    if is_submit:\n",
    "        pass\n",
    "    else:\n",
    "        base_data = pd.merge(base_data, label, on=['user_id', 'item_id'], how='left', copy=False)\n",
    "        base_data['label'] = base_data['label'].fillna(0)\n",
    "    print(base_data.shape)\n",
    "    base_data = base_data.fillna(0)\n",
    "#     del base_data['user_item']\n",
    "#     base_data = reduce_mem_usage(base_data)\n",
    "    return base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 12月17日 之前的历史信息全量\n",
    "train_label = pd.read_parquet(\"../sub_data/use_data/train_label.parquet\")\n",
    "train_item = pd.read_parquet(\"../sub_data/use_data/train_item.parquet\")\n",
    "user_data_list = {'2014-12-16': 0}\n",
    "train_history = get_history_data(train_label,train_item,user_data_list,'for_train',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label.head(),train_item.head(),train_label['label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_history = pd.read_parquet(path + \"/sub_data/use_data/history_for_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history['user_item'] = train_history[['user_id','item_id']].parallel_apply(lambda x:str(x[0]) +'_' + str(x[1]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history.dtypes,train_label.dtypes,train_item.dtypes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_history = reduce_mem_usage(train_history)\n",
    "# train_label = reduce_mem_usage(train_label)\n",
    "# train_item = reduce_mem_usage(train_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history.dtypes,train_label.dtypes,train_item.dtypes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_train_lgb = gen_feature(train_history,train_item,train_label,False,'train_o2o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_train_lgb.dtypes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_train_lgb['label'] = for_train_lgb['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_train_lgb['label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_train_lgb['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 12月18日 之前的历史信息全量\n",
    "valid_label = pd.read_parquet(\"../sub_data/use_data/valid_label.parquet\")\n",
    "valid_item = pd.read_parquet(\"../sub_data/use_data/valid_item.parquet\")\n",
    "user_data_list = {'2014-12-17': 0}\n",
    "valid_history = get_history_data(valid_label,valid_item,user_data_list,'for_valid',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_history = pd.read_parquet(path + \"/sub_data/use_data/history_for_valid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_history['user_item'] = valid_history[['user_id','item_id']].parallel_apply(lambda x:str(x[0]) +'_' + str(x[1]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_history = reduce_mem_usage(valid_history)\n",
    "# valid_label = reduce_mem_usage(valid_label)\n",
    "# valid_item = reduce_mem_usage(valid_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_valid_lgb = gen_feature(valid_history,valid_item,valid_label,False,'valid_o2o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del valid_history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_valid_lgb['label'] = for_valid_lgb['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for_valid_lgb.dtypes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_valid_lgb['label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for_valid_lgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_valid_lgb['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train', for_train_lgb.shape, 'valid', for_valid_lgb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nvugVO6oAMX"
   },
   "outputs": [],
   "source": [
    "feature = [x for x in for_train_lgb.columns if x not in ['user_category','gap' ,'label', 'user_geohash', 'predict', 'user_item','user_hour','item_hour','user_item_hour']]\n",
    "label = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_O4N8xCoCUW",
    "outputId": "97677316-78a6-45e6-e14c-6b13820f3922"
   },
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(for_train_lgb[feature], for_train_lgb[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gT8dmncdoHHd"
   },
   "outputs": [],
   "source": [
    "lgb_eval = lgb.Dataset(for_valid_lgb[feature], for_valid_lgb[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 1024,\n",
    "    'verbose': -1\n",
    "}\n",
    "callbacks = [lgb.log_evaluation(period=25), lgb.early_stopping(stopping_rounds=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "buYbip5jn3ZY",
    "outputId": "5454b9d6-701a-477b-a614-65743d419137"
   },
   "outputs": [],
   "source": [
    "print('Starting training...')\n",
    "# feature_name and categorical_feature\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10000,\n",
    "                valid_sets=[lgb_train, lgb_eval],\n",
    "                callbacks=callbacks,\n",
    "                )\n",
    "del lgb_train, lgb_eval\n",
    "gc.collect()\n",
    "# [106]\ttraining's auc: 0.902916\tvalid_1's auc: 0.891666\n",
    "# [107]\ttraining's auc: 0.91216\tvalid_1's auc: 0.90192\n",
    "# [109]\ttraining's auc: 0.91338\tvalid_1's auc: 0.905031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtVEFw4hUd5m"
   },
   "outputs": [],
   "source": [
    "feature_imp_df = pd.DataFrame()\n",
    "feature_imp_df['feature'] = gbm.feature_name()\n",
    "feature_imp_df['import'] = gbm.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "YiHkfyVWUhFV",
    "outputId": "11d25b69-ccc0-43c9-a5ac-9b58d93e0dfc"
   },
   "outputs": [],
   "source": [
    "feature_imp_df.sort_values('import')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_df.to_csv('./feature_imp_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5GgbgHnUe94"
   },
   "outputs": [],
   "source": [
    "for_valid_lgb['predict'] = gbm.predict(for_valid_lgb[feature], num_iteration=gbm.best_iteration).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_valid_lgb['predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_valid_label = pd.read_parquet(\"../sub_data/use_data/valid_label.parquet\")\n",
    "for_valid_label['user_item'] = for_valid_label[['user_id','item_id']].parallel_apply(lambda x:str(x[0]) +'_' + str(x[1]),axis=1)\n",
    "for_valid_label_set = set(for_valid_label['user_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_valid_ = for_valid_lgb.groupby(['user_id','item_id'])['predict'].mean().reset_index()\n",
    "for_valid_.columns = ['user_id','item_id','predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_valid_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = for_valid_.sort_values(['predict'])[-320000:]\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['user_item'] = t[['user_id','item_id']].parallel_apply(lambda x:str(x[0]) +'_' + str(x[1]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_and_for_valid_label = set(t['user_item']) & for_valid_label_set\n",
    "p = len(t_and_for_valid_label) / len(set(t['user_item']))\n",
    "r = len(t_and_for_valid_label) / len(for_valid_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((2 * p * r) / (p + r),p,r )\n",
    "# 0.07642381738543827 0.09337428614367298 0.06468194195477274\n",
    "# F1:0.0614  Recall:0.0528 Precision:0.0733\n",
    "# 0.07542477247013826 0.0980864981979542 0.061269239152712755\n",
    "# 0.07776773603280941 0.076234375 0.07936404657412137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 12月19日 之前的历史信息全量\n",
    "submit_item = pd.read_csv('../data/part1_item.txt', header=None, sep='\\t')\n",
    "submit_item.columns = ['item_id', 'item_geohash', 'item_category']\n",
    "user_data_list = {'2014-12-18': 0}\n",
    "submit_history =get_history_data('',submit_item,user_data_list,'for_submit',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit_history = pd.read_parquet(path + \"/sub_data/use_data/history_for_submit.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_history['user_item'] = submit_history[['user_id','item_id']].parallel_apply(lambda x:str(x[0]) +'_' + str(x[1]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit_history = reduce_mem_usage(submit_history)\n",
    "# submit_item = reduce_mem_usage(submit_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del submit_item['item_geohash']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_item.dtypes,submit_history.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_submit_lgb = gen_feature(submit_history,submit_item,'',True,'submit_o2o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del submit_history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del for_train_lgb;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for_train_valid_lgb = for_valid_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_train_valid_lgb = pd.concat([for_train_lgb,for_valid_lgb],copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del for_train_lgb,for_valid_lgb;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train_valid = lgb.Dataset(for_train_valid_lgb[feature], for_train_valid_lgb[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del for_train_valid_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting training...')\n",
    "# feature_name and categorical_feature\n",
    "gbm2 = lgb.train(params,\n",
    "                lgb_train_valid,\n",
    "                num_boost_round=int(gbm.best_iteration * 1.1),\n",
    "                valid_sets=[lgb_train_valid, lgb_train_valid],\n",
    "                callbacks=callbacks,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zeRn1gMG8o3k",
    "outputId": "2b6d91d2-1ae8-4055-fd0e-f5f26507106f"
   },
   "outputs": [],
   "source": [
    "# 提交结果\n",
    "for_submit_lgb['predict'] = gbm2.predict(for_submit_lgb[feature], num_iteration=gbm.best_iteration).reshape(-1, 1)\n",
    "print(for_submit_lgb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_ = for_submit_lgb.groupby(['user_id','item_id'])['predict'].mean().reset_index()\n",
    "submit_.columns = ['user_id','item_id','predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = submit_.sort_values('predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vxv2WE1_xeLC",
    "outputId": "32da2ef3-5a2d-4db0-e426-92cbb6fb13be"
   },
   "outputs": [],
   "source": [
    "submit = submit[['user_id','item_id','predict']][-35000:]\n",
    "print(submit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit[submit['item_id'].isin(list(submit_item['item_id'].unique()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit[['user_id','item_id']].to_csv(path + '/submit_35000_blind_1_1.txt', sep='\\t', index=False, header=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "submit[['user_id','item_id']].to_csv((\"../submit/submit_\"+datetime.datetime.now().strftime('%Y%m%d_%H%M%S') + \".csv\"), header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
